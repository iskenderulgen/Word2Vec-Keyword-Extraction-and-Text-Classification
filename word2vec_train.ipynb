{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec, Word2VecModel\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# To simulate a cluster environment, change the instance size to test multi instance performance\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Spark-Word2Vec\")\n",
    "    .config(\"spark.driver.memory\", \"20g\")\n",
    "    #.config(\"spark.driver.cores\", \"2\")\n",
    "    #.config(\"spark.executor.cores\", \"2\")\n",
    "    #.config(\"spark.executor.memory\", \"2g\")\n",
    "    #.config(\"spark.driver.maxResultSize\", \"3g\")\n",
    "    #.config(\"spark.executor.instances\", \"2\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Read and Clean the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON lines file\n",
    "comments = (\n",
    "    spark.read.json(\"data/RC_2010-07\")\n",
    "    .select(\"body\", \"subreddit\")\n",
    "    .where(\"body != '[deleted]' AND body != '[removed]'\")\n",
    "    # Replace newline and carriage return characters with a space\n",
    "    .withColumn(\"body\", F.regexp_replace(F.col(\"body\"), \"[\\\\r\\\\n]+\", \" \"))\n",
    "    # Remove URLs (matches strings starting with http or https)\n",
    "    .withColumn(\"body\", F.regexp_replace(F.col(\"body\"), \"https?://\\\\S+\", \"\"))\n",
    "    # Remove characters that are not letters, digits, whitespace, or apostrophes\n",
    "    .withColumn(\"body\", F.regexp_replace(F.col(\"body\"), \"[^a-zA-Z0-9\\\\s']\", \"\"))\n",
    ")\n",
    "\n",
    "comments.show(5, truncate=50)\n",
    "\n",
    "# Get basic statistics\n",
    "print(f\"Number of records: {comments.count()}\")\n",
    "print(f\"Number of columns: {len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-Create a pipeline to tokenize the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline stages\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"tokens\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "\n",
    "# Create and fit the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover])\n",
    "comments = pipeline.fit(comments).transform(comments).select(\"filtered_tokens\")\n",
    "\n",
    "comments.show(5, truncate=50)\n",
    "\n",
    "print(f\"Number of total words to train Word2Vec: {comments.select(F.explode('filtered_tokens')).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model, depending on the size of the data, this may take a while\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    vectorSize=50,\n",
    "    minCount=5,\n",
    "    maxIter=1,\n",
    "    inputCol=\"filtered_tokens\",\n",
    "    outputCol=\"word2vec_features\",\n",
    ").fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "word2vec_model.write().overwrite().save(\"data/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "word2vec_model = Word2VecModel.load(\"data/word2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1-Extract Keywords from Similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms for a list of words for later training\n",
    "\n",
    "words_list = [\"music\", \"gaming\", \"politics\", \"programming\", \"science\"]\n",
    "keywords = []\n",
    "\n",
    "for word in words_list:\n",
    "    synonyms = word2vec_model.findSynonyms(word, 50).toPandas().drop(columns=\"similarity\")\n",
    "    synonyms[\"label\"] = word\n",
    "    keywords.append(synonyms)\n",
    "\n",
    "keywords = pd.concat(keywords, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.to_parquet(\"data/keywords.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-word2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
